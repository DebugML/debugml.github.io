---
title: "The FIX Benchmark: Extracting Features Interpretable to eXperts"
layout: single
excerpt: "A benchmark for evaluating how interpretable features are to real-world expert, ranging from gallbladder surgeons to supernova cosmologists."

header:
  overlay_filter: "0.75"
  overlay_image: /assets/images/fix/IF_extraction.png
  teaser: /assets/images/fix/IF_extraction.png
  actions:
    - label: "Paper"
      ulr: https://arxiv.org/abs/2409.13684
    - label: "Code"
      url: https://github.com/BrachioLab/exlib/tree/main/fix

authors:
  - Helen Jin
  - Shreya Havaldar
  - Chaehyeon Kim
  - Anton Xue
  - Weiqiu You
  - Helen Qu
  - Marco Gatti
  - Daniel A. Hashimoto
  - Bhuvnesh Jain
  - Amin Madani
  - Masao Sako
  - Lyle Ungar
  - Eric Wong


gallery_IF_extraction:
  - image_path: /assets/images/fix/IF_extraction.png
    title: The FIX benchmark measure the alignment of features with expert knowledge.

gallery_fix_overview:
  - image_path: /assets/images/fix/fix_overview.png
    title: Overview of the FIX benchmark's datasets.


---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



> Feature-based methods are commonly used to explain model predictions, but the explanations are often not interpretable.
> Can we instead automatically extract collections or groups of features that are aligned with expert knowledge?
> In collaboration with domain experts towards this goal, we present [**FIX**](https://brachiolab.github.io/fix/), a benchmark for evaluating how well feature-based explanations are interpretable to the experts.


{% include gallery id="gallery_IF_extraction" caption="The FIX benchmark measures the alignment of given features with respect to expert knowledge, which may be either explicitly specified as labels or implicitly given as a scoring function." %}


Machine learning models are increasingly used in domains like
[healthcare](https://pubs.rsna.org/doi/full/10.1148/ryai.2020190043),
[law](https://www.sciencedirect.com/science/article/pii/S0004370220301375),
[governance](https://www.tandfonline.com/doi/full/10.1080/01900692.2019.1575664),
[science](https://link.springer.com/article/10.1007/s00607-023-01181-x),
[education](https://link.springer.com/book/10.1007/978-3-319-93843-1),
and [finance](https://arxiv.org/abs/1811.06471).
Although state-of-the-art models attain good performance, domain experts rarely not trust them because the underlying algorithms are black-box.
This opaqueness is a liability where transparency is crucial, especially in domains such as healthcare and law, where experts need **explainability** to ensure the safe and effective use of machine learning.



The need for transparent and explainable models has emerged as a central research focus 
One popular approach is to explain model behavior in terms of the input features, i.e., the pixels of an image or the tokens of a prompt.
**However, feature-based explanation methods often do not produce interpretable explanations.**
The main challenge is that feature-based explanations commonly assume that the given features are already interpretable to the user, but this typically only holds for low-dimensional data.
With high-dimensional data like images and documents, features at the granularity of pixels and tokens may lack enough salient semantic information to be meaningfully understood even by experts.
Moreover, the features relevant for an explanation are often domain-dependent, which means that experts of different domains will care about different features.
These factors limit the usability of popular, general-purpose feature-based explanation techniques on high-dimensional data.


Instead of individual features, users often understand high-dimensional data in terms of semantic collections of low-level features, such as regions of an image or phrases in a document.
Moreover, for a feature to be useful, it should align with the intuition of **domain experts** in the field.
That is, an interpretable feature for high-dimensional data should satisfy the following:

1. Encompass a grouping of related low-level features, e.g., pixels, tokens, to create a meaningful high-level feature.
2. These groupings should align with domain expert knowledge of the relevant task.

This raises a critical question:

> Can we automatically extract features that are aligned with domain-expert knowledge?


Towards this goal, we present [**FIX**](https://brachiolab.github.io/fix/), a benchmark for measuring the interpretability of features with respect to expert knowledge.
FIX is developed in collaboration with domain experts spanning gallbladder surgeons to supernova cosmologists.
An overview of FIX is shown in the following table.


{% include gallery id="gallery_fix_overview" caption="An overview of the datasets available in the FIX benchmark." %}


Check out FIX here: [https://brachiolab.github.io/fix/](https://brachiolab.github.io/fix/)

```
@article{jin2024fix,
  title={The FIX Benchmark: Extracting Features Interpretable to eXperts}, 
  author={Jin, Helen and Havaldar, Shreya and Kim, Chaehyeon and Xue, Anton and You, Weiqiu and Qu, Helen and Gatti, Marco and Hashimoto, Daniel and Jain, Bhuvnesh and Madani, Amin and Sako, Masao and Ungar, Lyle and Wong, Eric},
  journal={arXiv preprint arXiv:2409.13684},
  year={2024},
}
```