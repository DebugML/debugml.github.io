---
title: "The FIX Benchmark: Extracting Features Interpretable to eXperts"
layout: single
excerpt: "We present a benchmark for evaluating how interpretable features are to real-world experts, ranging from gallbladder surgeons to supernova cosmologists."

header:
  overlay_filter: "0.75"
  overlay_image: /assets/images/fix/dog_predict_color.png
  teaser: /assets/images/fix/dog_predict_color.png
  actions:
    - label: "Paper"
      ulr: https://arxiv.org/abs/2409.13684
    - label: "Code"
      url: https://github.com/BrachioLab/exlib/tree/main/fix

authors:
  - Helen Jin
  - Shreya Havaldar
  - Chaehyeon Kim
  - Anton Xue
  - Weiqiu You
  - Helen Qu
  - Marco Gatti
  - Daniel A. Hashimoto
  - Bhuvnesh Jain
  - Amin Madani
  - Masao Sako
  - Lyle Ungar
  - Eric Wong


gallery_IF_extraction:
  - url: /assets/images/fix/IF_extraction.png
    image_path: /assets/images/fix/IF_extraction.png
    title: The FIX benchmark measure the alignment of features with expert knowledge.

gallery_fix_overview:
  - url: /assets/images/fix/fix_overview.png 
    image_path: /assets/images/fix/fix_overview.png
    title: Overview of the FIX benchmark's datasets.

gallery_cholec_image:
  - url: /assets/images/fix/cholec_image.png
    image_path: /assets/images/fix/cholec_image.png
    title: Full image
  - url: /assets/images/fix/cholec_gng_safe.png
    image_path: /assets/images/fix/cholec_gng_safe.png
    title: Safe
  - url: /assets/images/fix/cholec_gng_unsafe.png
    image_path: /assets/images/fix/cholec_gng_unsafe.png
    title: Unsafe

# gallery_chestx_image:
#   - url: /assets/images/fix/chestx_image.png
#     image_path: /assets/images/fix/chestx_image.png
#     title: Full image
#   - url: /assets/images/fix/chestx_Right_Lung.png
#     image_path: /assets/images/fix/chestx_Right_Lung.png
#     title: Right lung
#   - url: /assets/images/fix/chestx_Left_Lung.png
#     image_path: /assets/images/fix/chestx_Left_Lung.png
#     title: Left lung



---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



{% include gallery id="gallery_IF_extraction" caption="The FIX benchmark measures the alignment of given features with respect to expert knowledge, which may be either explicitly specified as labels or implicitly given as a scoring function." %}


Machine learning models are increasingly used in domains like
[healthcare](https://pubs.rsna.org/doi/full/10.1148/ryai.2020190043),
[law](https://www.sciencedirect.com/science/article/pii/S0004370220301375),
[governance](https://www.tandfonline.com/doi/full/10.1080/01900692.2019.1575664),
[science](https://link.springer.com/article/10.1007/s00607-023-01181-x),
[education](https://link.springer.com/book/10.1007/978-3-319-93843-1),
and [finance](https://arxiv.org/abs/1811.06471).
Although state-of-the-art models attain good performance, domain experts rarely not trust them because the underlying algorithms are black-box.
This opaqueness is a liability where transparency is crucial, especially in domains such as healthcare and law, where experts need **explainability** to ensure the safe and effective use of machine learning.



The need for transparent and explainable models has emerged as a central research focus 
One popular approach is to explain model behavior in terms of the input features, i.e., the pixels of an image or the tokens of a prompt.
**However, feature-based explanation methods often do not produce interpretable explanations.**
The main challenge is that feature-based explanations commonly assume that the given features are already interpretable to the user, but this typically only holds for low-dimensional data.
With high-dimensional data like images and documents, features at the granularity of pixels and tokens may lack enough salient semantic information to be meaningfully understood even by experts.
Moreover, the features relevant for an explanation are often domain-dependent, which means that experts of different domains will care about different features.
These factors limit the usability of popular, general-purpose feature-based explanation techniques on high-dimensional data.


Instead of individual features, users often understand high-dimensional data in terms of semantic collections of low-level features, such as regions of an image or phrases in a document.
Moreover, for a feature to be useful, it should align with the intuition of **domain experts** in the field.
That is, an interpretable feature for high-dimensional data should satisfy the following properties:

1. Encompass a grouping of related low-level features, e.g., pixels, tokens, to create a meaningful high-level feature.
2. These groupings should align with domain expert knowledge of the relevant task.

We refer to features that satisfy these criteria as **expert features**. In other words, an expert feature is a high-level feature that experts in the domain find semantically meaningful and useful.

A critical question follows:

> Can we automatically discover expert features that align with domain knowledge?

## The FIX Benchmark 
Towards this goal, we present [**FIX**](https://brachiolab.github.io/fix/), a benchmark for measuring the interpretability of features with respect to expert knowledge. We work closely with with domain experts, spanning gallbladder surgeons to supernova cosmologists, to define criteria for interpretability of features in each domain. 

An overview of FIX is shown in the following table:

{% include gallery id="gallery_fix_overview" caption="An overview of the datasets available in the FIX benchmark." %}


## Example of Expert Features (Cholecystectomy)
For example, in cholecystectomy (gallbladder removal surgery), surgeons may consider vital organs and structures (such as the liver, gallbladder, hepatocystic triangle) as expert features. 

{% include gallery id="gallery_cholec_image" caption="[left] The view of the surgeon sees; [middle] The safe region for operations; [right] The gallbladder, a
key anatomical structure for the critical view of safety." %}


These organ segmentations are expert features because experts use them when making predictions for which regions are safe to operate on in their critical view of safety, before performing any irreversible operations. To evaluate a set of candidate features $\hat G$ for an example $x$, we define the following FIXScore:

$$\begin{align*}
    \mathsf{FIXScore}(\hat{G}, x) =
    \frac{1}{d} \sum_{i = 1}^{d}
    \underset{\hat{g} \in \hat{G}[i]}{\mathbb{E}}\,
    \Big[\mathsf{ExpertAlign}(\hat{g}, x)\Big]
\end{align*}$$

where
$$\hat{G}[i] = \{\hat{g} : \text{group \(\hat{g}\) includes feature \(i\)}\}$$ 
and $\mathsf{ExpertAlign}$ measures how well a proposed feature $\hat g$ aligns with the experts' judgment. In the Cholecystectomy setting, we have existing ground truth annotations $G^\star$ from experts. We can thus evaluate the proposed features with the explicit metric of intersection-over-union (IOU) between the proposed feature $\hat{g}$ and the ground truth annotations $G^\star$ as follows:

$$\mathsf{ExpertAlign} (\hat{g}, x) =  \max_{g^{\star} \in G^{\star}} \frac{|\hat{g} \cap g^\star|}{|\hat{g} \cup g^\star|}$$

<!-- ## Example of Expert Features (Chest X-Ray)
For example, a radiologist might consider anatomical structures in a Chest X-Ray such as the left and right lungs as expert features. 

{% include gallery id="gallery_chestx_image" caption="[left] The full X-ray image where the following pathologies are present: effusion, infiltration,
and pneumothorax; [middle, right] Expert-interpretable anatomical structures of the left and right lungs" %}

These anatomical structures are expert features because experts use them when making predictions for pathologies. To evaluate a set of candidate features $\hat G$ for an example $x$, we define the following FIXScore:

$$\begin{align*}
    \mathsf{FIXScore}(\hat{G}, x) =
    \frac{1}{d} \sum_{i = 1}^{d}
    \underset{\hat{g} \in \hat{G}[i]}{\mathbb{E}}\,
    \Big[\mathsf{ExpertAlign}(\hat{g}, x)\Big]
\end{align*}$$

where
$$\hat{G}[i] = \{\hat{g} : \text{group \(\hat{g}\) includes feature \(i\)}\}$$ 
and $\mathsf{ExpertAlign}$ measures how well a proposed feature $\hat g$ aligns with the experts' judgment. In the Chest X-Ray setting, we have existing ground truth annotations $G^\star$ from experts. We can thus evaluate the proposed features with the explicit metric of intersection-over-union (IOU) between the proposed feature $\hat{g}$ and the ground truth annotations $G^\star$ as follows:

$$\mathsf{ExpertAlign} (\hat{g}, x) =  \max_{g^{\star} \in G^{\star}} \frac{|\hat{g} \cap g^\star|}{|\hat{g} \cup g^\star|}$$ -->

---
To explore more settings, check out FIX here: [https://brachiolab.github.io/fix/](https://brachiolab.github.io/fix/)


## Citation
Thank you for stopping by! 

Please cite our work if you find it helpful.
```bibtex
@article{jin2024fix,
  title={The FIX Benchmark: Extracting Features Interpretable to eXperts}, 
  author={Jin, Helen and Havaldar, Shreya and Kim, Chaehyeon and Xue, Anton and You, Weiqiu and Qu, Helen and Gatti, Marco and Hashimoto, Daniel and Jain, Bhuvnesh and Madani, Amin and Sako, Masao and Ungar, Lyle and Wong, Eric},
  journal={arXiv preprint arXiv:2409.13684},
  year={2024}
}
```