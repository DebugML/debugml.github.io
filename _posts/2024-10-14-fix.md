---
title: "The FIX Benchmark: Extracting Features Interpretable to eXperts"
layout: single
excerpt: "We present the FIX benchmark for evaluating how interpretable features are to real-world experts, ranging from gallbladder surgeons to supernova cosmologists."

header:
  overlay_filter: "0.75"
  overlay_image: /assets/images/fix/dog_predict_color.png
  teaser: /assets/images/fix/dog_predict_color.png
  actions:
    - label: "Paper"
      url: https://arxiv.org/abs/2409.13684
    - label: "Code"
      url: https://github.com/BrachioLab/exlib/tree/main/fix

authors:
  - Helen Jin
  - Shreya Havaldar
  - Chaehyeon Kim
  - Anton Xue
  - Weiqiu You
  - Helen Qu
  - Marco Gatti
  - Daniel A. Hashimoto
  - Bhuvnesh Jain
  - Amin Madani
  - Masao Sako
  - Lyle Ungar
  - Eric Wong


gallery_IF_extraction:
  - url: /assets/images/fix/IF_extraction.png
    image_path: /assets/images/fix/IF_extraction.png
    title: The FIX benchmark measures the alignment of features to domain expert knowledge.

gallery_fix_overview:
  - url: /assets/images/fix/fix_overview.png 
    image_path: /assets/images/fix/fix_overview.png
    title: Overview of the FIX benchmark's datasets.

gallery_cholec_image:
  - url: /assets/images/fix/raw_image.png
    image_path: /assets/images/fix/blr_image.png
    title: Full View of Surgery.
  - url: /assets/images/fix/gng_raw_masked_1.png
    image_path: /assets/images/fix/gng_blr_masked_1.png
    title: Safe area for operation.
  - url: /assets/images/fix/exp_raw_masked_2.png
    image_path: /assets/images/fix/exp_blr_masked_2.png
    title: The gallbladder, a key anatomical structure for the critical view of safety.

# gallery_cholec_image:
#   - url: /assets/images/fix/cholec_image.png
#     image_path: /assets/images/fix/cholec_image.png
#     title: Full View of Surgery.
#   - url: /assets/images/fix/cholec_gng_safe.png
#     image_path: /assets/images/fix/cholec_gng_safe.png
#     title: Safe area for operation.
#   - url: /assets/images/fix/cholec_gng_unsafe.png
#     image_path: /assets/images/fix/cholec_gng_unsafe.png
#     title: Unsafe area for operation.


# gallery_chestx_image:
#   - url: /assets/images/fix/chestx_image.png
#     image_path: /assets/images/fix/chestx_image.png
#     title: Full image
#   - url: /assets/images/fix/chestx_Right_Lung.png
#     image_path: /assets/images/fix/chestx_Right_Lung.png
#     title: Right lung
#   - url: /assets/images/fix/chestx_Left_Lung.png
#     image_path: /assets/images/fix/chestx_Left_Lung.png
#     title: Left lung



---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

> Explanations for machine learning need interpretable features, but current methods fall short of discovering them.
> Intuitively, interpretable features should align with domain-specific expert knowledge.
> Can we measure the interpretability of such features and in turn automatically find them?
> In this blog post, we delve into our joint work with domain experts in creating the [**FIX**](https://brachiolab.github.io/fix/) benchmark, which directly evaluates the interpretability of features in real world settings, ranging from psychology to cosmology.

Machine learning models are increasingly used in domains like
[healthcare](https://pubs.rsna.org/doi/full/10.1148/ryai.2020190043),
[law](https://www.sciencedirect.com/science/article/pii/S0004370220301375),
[governance](https://www.tandfonline.com/doi/full/10.1080/01900692.2019.1575664),
[science](https://link.springer.com/article/10.1007/s00607-023-01181-x),
[education](https://link.springer.com/book/10.1007/978-3-319-93843-1),
and [finance](https://arxiv.org/abs/1811.06471).
Although state-of-the-art models attain good performance, domain experts rarely not trust them because the underlying algorithms are black-box.
This opaqueness is a liability where transparency is crucial, especially in domains such as healthcare and law, where experts need **explainability** to ensure the safe and effective use of machine learning.



The need for transparent and explainable models has emerged as a central research focus. 
One popular approach is to explain model behavior in terms of the input features, i.e. the pixels of an image or the tokens of a prompt.
However, feature-based explanation methods often do not produce interpretable explanations.
**The main challenge is that feature-based explanations commonly assume that the given features are already interpretable to the user, but this is typically only true for low-dimensional data.**
With high-dimensional data like images and documents, features at the granularity of pixels and tokens may lack enough salient semantic information to be meaningfully understood even by experts.
Moreover, the features relevant for an explanation are often domain-dependent, which means that experts of different domains will care about different features.
These factors limit the usability of popular, general-purpose feature-based explanation techniques on high-dimensional data.

{% include gallery id="gallery_IF_extraction" caption="The FIX benchmark measures the alignment of given features with respect to expert knowledge, which may be either explicitly specified as labels or implicitly given as a scoring function." %}

Instead of individual features, users often understand high-dimensional data in terms of semantic collections of low-level features, such as regions of an image or phrases in a document. In the figure above, a pixel as a feature would not be very informative, but rather the pixels that make up a dog in the image would make more sense to a user.
Furthermore, for a feature to be useful, it should align with the intuition of domain experts in the field.
That is, an interpretable feature for high-dimensional data should satisfy the following properties:

1. Encompass a grouping of related low-level features, e.g., pixels, tokens, to create a meaningful high-level feature.
2. These groupings should align with domain expert knowledge of the relevant task.

We refer to features that satisfy these criteria as **expert features**. In other words, an expert feature is a high-level feature that experts in the domain find semantically meaningful and useful.

A critical question follows: 

<i> Can we automatically discover expert features that align with domain knowledge? </i>

## The FIX Benchmark 
Towards this goal, we present [**FIX**](https://brachiolab.github.io/fix/), a benchmark for measuring the interpretability of features with respect to expert knowledge. We work closely with with domain experts, spanning gallbladder surgeons to supernova cosmologists, to define criteria for interpretability of features in each domain. 

An overview of FIX is shown in the following table below. We showcase 6 different real-world settings spanning cosmology, psychology and medicine. These settings cover 3 different data modalities (image, text, and time series). We delineate what the input and output of each setting's dataset are, as well as the size of the dataset and what experts consider to be good features (i.e. expert features). The main merit of FIX is that it unifies all of these different settings into a single framework by providing a straightforward way to measure a feature's alignment with expert knowledge. 

{% include gallery id="gallery_fix_overview" caption="An overview of the datasets available in the FIX benchmark." %}


## Example of Expert Features (Cholecystectomy)
For example, in cholecystectomy (gallbladder removal surgery), surgeons may consider vital organs and structures (such as the liver, gallbladder, hepatocystic triangle) as expert features. 


<b> [Warning!] </b> Clicking on a blurred image below will show the unblurred version of the image. This depicts the actual surgery which can be graphic in nature. Please click at your own discretion. 
{: .notice--danger}

{% include gallery id="gallery_cholec_image" caption="[Left] The view of the surgeon sees; [Middle] The safe region for operation; [Right] The gallbladder, a key anatomical structure for the critical view of safety." %}


These organ segmentations are expert features because experts use them when making predictions for which regions are safe to operate on in their critical view of safety, before performing any irreversible operations. To evaluate a set of candidate features $\hat G$ for an example $x$, we define the following FIXScore:

$$\begin{align*}
    \mathsf{FIXScore}(\hat{G}, x) =
    \frac{1}{d} \sum_{i = 1}^{d}
    \underset{\hat{g} \in \hat{G}[i]}{\mathbb{E}}\,
    \Big[\mathsf{ExpertAlign}(\hat{g}, x)\Big]
\end{align*}$$

where
$$\hat{G}[i] = \{\hat{g} : \text{group \(\hat{g}\) includes feature \(i\)}\}$$ 
and $\mathsf{ExpertAlign}$ measures how well a proposed feature $\hat g$ aligns with the experts' judgment. In the Cholecystectomy setting, we have existing ground truth annotations $G^\star$ from experts. We can thus evaluate the proposed features with the **explicit** metric of intersection-over-union (IOU) between the proposed feature $\hat{g}$ and the ground truth annotations $G^\star$ as follows:

$$\mathsf{ExpertAlign} (\hat{g}, x) =  \max_{g^{\star} \in G^{\star}} \frac{|\hat{g} \cap g^\star|}{|\hat{g} \cup g^\star|}.$$

### Implicit Expert Features
However, note that ground truth annotations are not always available in our settings, and in those such cases, we can instead implicity specify expert knowledge through an expert alignment scoring function. For example, in the multilingual politeness setting, the scoring function would specifically measure how closely the text features align with the lexical categories for politeness.

<!-- ## Example of Expert Features (Chest X-Ray)
For example, a radiologist might consider anatomical structures in a Chest X-Ray such as the left and right lungs as expert features. 

{% include gallery id="gallery_chestx_image" caption="[left] The full X-ray image where the following pathologies are present: effusion, infiltration,
and pneumothorax; [middle, right] Expert-interpretable anatomical structures of the left and right lungs" %}

These anatomical structures are expert features because experts use them when making predictions for pathologies. To evaluate a set of candidate features $\hat G$ for an example $x$, we define the following FIXScore:

$$\begin{align*}
    \mathsf{FIXScore}(\hat{G}, x) =
    \frac{1}{d} \sum_{i = 1}^{d}
    \underset{\hat{g} \in \hat{G}[i]}{\mathbb{E}}\,
    \Big[\mathsf{ExpertAlign}(\hat{g}, x)\Big]
\end{align*}$$

where
$$\hat{G}[i] = \{\hat{g} : \text{group \(\hat{g}\) includes feature \(i\)}\}$$ 
and $\mathsf{ExpertAlign}$ measures how well a proposed feature $\hat g$ aligns with the experts' judgment. In the Chest X-Ray setting, we have existing ground truth annotations $G^\star$ from experts. We can thus evaluate the proposed features with the explicit metric of intersection-over-union (IOU) between the proposed feature $\hat{g}$ and the ground truth annotations $G^\star$ as follows:

$$\mathsf{ExpertAlign} (\hat{g}, x) =  \max_{g^{\star} \in G^{\star}} \frac{|\hat{g} \cap g^\star|}{|\hat{g} \cup g^\star|}$$ -->



---
To explore more settings, check out FIX here: [https://brachiolab.github.io/fix/](https://brachiolab.github.io/fix/)


## Citation
Thank you for stopping by! 

Please cite our work if you find it helpful.
```bibtex
@article{jin2024fix,
  title={The FIX Benchmark: Extracting Features Interpretable to eXperts}, 
  author={Jin, Helen and Havaldar, Shreya and Kim, Chaehyeon and Xue, Anton and You, Weiqiu and Qu, Helen and Gatti, Marco and Hashimoto, Daniel and Jain, Bhuvnesh and Madani, Amin and Sako, Masao and Ungar, Lyle and Wong, Eric},
  journal={arXiv preprint arXiv:2409.13684},
  year={2024}
}
```