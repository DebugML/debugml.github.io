---
title: "CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning"
layout: single
excerpt: "Scaling neurosymbolic learning with program decomposition and tensor sketching."
header:
  overlay_color: "#000"
  overlay_filter: "0.75"
  overlay_image: assets/images/ctsketch/overview-white.png
  teaser: assets/images/ctsketch/overview.png
  actions:
    - label: "Paper"
      url: https://arxiv.org/abs/XXXX.XXXX
    - label: "Code"
      url: https://github.com/alaiasolkobreslin/CTSketch
authors:
  - Seewon Choi|equal
  - Alaia Solko-Breslin|equal
  - Rajeev Alur
  - Eric Wong

scene_recognition:
  - url: /assets/images/ctsketch/scene.png
    image_path: /assets/images/ctsketch/scene.png
    title: Scene recognition can be decomposed as an object detector followed by a call to GPT-4 to classify the scene.

task_decompositions:
  - id: 0
    name: sum
    caption: Program Decomposition for MNIST sum of n digits
  - id: 1
    name: add
    caption: Program Decomposition for MNIST addition of 2 n-digit numbers
  - id: 2
    name: visudo
    caption: Program Decomposition for Visual Sudoku
  - id: 3
    name: sudoku
    caption: Program Decomposition for Sudoku Solving

---
<style>
.histogram-row {
    display: flex;
    justify-content: space-between;
    flex-wrap: nowrap;
}

.histogram-row > * {
    flex: 0 0 48%; /* this ensures the child takes up 48% of the parent's width (leaving a bit of space between them) */
}

.button-method {
  width: 25%;
  background: rgba(76, 175, 80, 0.0);
  border: 0px;
  border-right: 1px solid #ccc;
  color: #999;
}

.button-sample {
  padding: 5px;
  font-size: 12px;
  background: rgba(76, 175, 80, 0.0);
  display: inline-block;
  margin-right: 15px;
}

.btn-clicked {
  color: black;
}

.container {
  display: flex;
  overflow: auto;
  align-items: center;
}

.container th, .container td {
  text-align: center;
  padding: 1px 5px;
}

.container table {
  width: auto; 
  padding-top:15px;
  margin-right: 5px;
}

.container math, .container div {
  width: auto; 
  margin-right: 15px;
}

.container div {
  margin-left: 15px;
}

.code-block {
  font-size: 14px; /* Adjust the font size as needed */
  text-align: left;
}

.code-snippet {
  display: inline-block;
  margin-left: 15px;
  margin-right: 15px;
}

</style>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.4/Chart.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>


> This post introduces CTSketch, an algorithm for learning tasks expressed as the composition of a neural network followed by a symbolic program (neurosymbolic learning).
> CTSketch decomposes the symbolic program, use tensors to summarize the input-output pairs of each sub-program, and performs fast inference via simple tensor operations.
> CTSketch pushes the frontier of neursymbolic learning, scaling to tasks involving 1024 inputs, which has never been studied before. 

Neurosymbolic programs are the composition of a neural network $M_\theta$ followed by a fixed symbolic program $c$.
The challenge of neurosymbolic learning concerns learning $M_\theta$ without intermediate labels.
Concretely, given an input example $(x, y)$, symbolic program $c$, and loss function $\mathcal{L}$, we must predict the distribution for the output ${y'} = c(M_{\theta}(x))$ while ensuring thta the loss $\mathcal{L}({y'}, y)$ is fully differentiable.
We now discuss prior works on neurosymbolic learning which provide different techniques for estimating the output distribution of the composite.

## White- and Black-Box Neurosymbolic Frameworks

Existing neurosymbolic learning techniques can usually be categorized as either white- or black-box.
White-box techniques allow access to the internals of the symbolic program in order to compute and output probability distribution from the input distributions.
Differentiable logic programming frameworks are one form of white-box learning techniques.
These frameworks require the symbolic component to be written in a logic programming language like probabilistic Prolog (as in [DeepProbLog](https://arxiv.org/abs/1805.10872)) or Datalog (as in [Scallop](https://arxiv.org/abs/2304.04812)).

On the other hand, black-box frameworks assume no access to the internals of the symbolic component.
[IndeCateR](https://arxiv.org/abs/2311.12569) and [ISED](https://arxiv.org/abs/2406.06246) are two frameworks that sample inputs to the symbolic program and reward inputs that result in the correct output.
[A-NeSI](https://arxiv.org/abs/2212.12393) is an algorithm for approximating the output distribution of the composite with a neural network.
Because these techniques are suited for black-box programs, the symbolic component can be written in any programming language.
This means that the symbolic component can even use a large language model (LLM) like GPT-4 to perform reasoning over the predictions from the network.
For example, the task of scene recognition has a natural decomposition of an object detector followed by a program that prompts GPT-4 to classify the scene based on these objects.

{% include gallery id="scene_recognition" caption="Scene recognition can be decomposed as an object detector followed by a program that prompts GPT-4 to classify the scene based on the predicted objects." %}

## CTSketch

We introduce CTSketch, a novel framework that combines the strengths of white- and black-box techniques.
CTSketch uses two techniques to improve the scalability of neurosymbolic inference: decompose the program into multiple sub-programs and summarize each program with a sketched tensor.

**Program Decomposition.**
While CTSketch can learn black-box programs, its scalability benefits from program decomposition
We can manually specify sub-programs that form a tree structure with $m$ layers.
Each program is summarized as a tensor, where taking the product of the tensor summary and the corresponding input probabilities return probability distribution of the outputs of $c$. 
This computation is done sequentially from layers 1 to $m$, whereas computations within a layer can be parallelized. 

Different examples of program decomposition:
<!-- Decomposition Figure -->
<ul class="tab" data-tab="decomposition-examples" data-name="decompexample" style="margin-left:3px">
{% for i in (0..3) %}
<li class="{% if forloop.first %} active{% endif %}" style="width: 15%; height: 50px; padding: 0; margin: 0">
    <a href="#" style="padding: 5%; margin: 0"><img src="/assets/images/ctsketch/blog_figs_attrs/{{ i }}/thumbnail.png" alt="{{ i | plus: 1 }}"></a>
</li>
{% endfor %}
</ul>
<ul class="tab-content" id="decomposition-examples" data-name="decompexample">

{% for example in page.task_decompositions %}
<li class="{% if forloop.first %}active{% endif %}">
    <!-- Masked Images - First Row -->
    <div style="text-align: center; display: flex; justify-content: space-around; align-items: center;">
      {% if forloop.index <= 5 %}
      <figure class="center" style="margin: 0;">
          <a href="/assets/images/ctsketch/blog_figs_attrs/{{ example.id }}/{{ example.name }}.png" title="Example {{ forloop.parentloop.index }}" class="image-popup">
              <img src="/assets/images/ctsketch/blog_figs_attrs/{{ example.id }}/{{ example.name }}.png" alt="Masked Image {{ forloop.index }} for {{ forloop.parentloop.index }}" style="width: 95%">
          </a>
          <figcaption>{{ example.caption }}</figcaption>
      </figure>
      {% endif %}
    </div>
</li>
{% endfor %}
</ul>

**Tensor Sketching.**
To reduce the size of the tensor summaries and increase the efficinecy of inference, we use low-rank tensor decomposition methods. These techniques find low-rank tensors that reconstruct the original tensor with low error.
In particular, we use Tensor-Train factorization. 

**Algorithm**

Prior to training, CTSketch initializes the tneosr, either by sampling a subset of the possible inputs or enumerating all input combinations and computing the corresponding outputs. 
After initializing, the algrithm uses sketching method to obtain cores.

Inference proceeds by going layer-by-layer through the program, and approximating the expected value of the output by taking the product of the cores and input distributions. 
Then, we apply RBF kernel and $L_1$ normalization to transform the expected value into a distribution. 



The resulting distribution is passed on to the next layers as inputs, and this process repeats until the final layer produces the final output. 
Note that the final output need not be transformed and hence the final output space can be infinite, like floating-point numbers. 
During test time, instead of tensor sketches, we use the symbolic program $c$ with the argmax inputs. 

<!-- scales to tasks involving many inputs  -->

## Evaluation
We use tasks: sum-$n$ ($n \in$ {4, 16, 64, 256, 1024}), add-$n$ ($n \in$ {1, 2, 4, 15, 100}), visual Sudoku, Sudoku solving, Hand-Written Formula (HWF), scene recognition and leaf classification (involving calls to GPT-4).
We use Scallop, DeepSoftLog, IndeCateR, ISED, and A-NeSI as baselines. 

**Performance and Accuracy**
<body>
  <!-- 
    <button id="sumButton" style="background-color: lightgrey" onclick="showCustomTable()">Sum-N</button>
    <button id="addButton" style="background-color: lightgrey" onclick="showMnistArithTable()">Add-N</button> 
  -->
    <table id="sumTable" class="styled-table" style="margin-top: 5px;">
        <thead>
            <tr>
                <th></th>
                <th>sum-4</th>
                <th>sum-16</th>
                <th>sum-64</th>
                <th>sum-256</th>
                <th>sum-1024</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <th>Scallop</th>
                <td>88.90</td>
                <td>8.43</td>
                <td>TO</td>
                <td>TO</td>
                <td>TO</td>
            </tr>
            <tr>
                <th>DeepSoftLog</th>
                <td><strong>94.13</strong></td>
                <td>2.19</td>
                <td>TO</td>
                <td>TO</td>
                <td>TO</td>
            </tr>
            <tr>
                <th>IndeCateR</th>
                <td>92.55</td>
                <td>83.01</td>
                <td>44.43</td>
                <td>0.51</td>
                <td>0.60</td>
            </tr>
            <tr>
                <th>ISED</th>
                <td>90.79</td>
                <td>73.50</td>
                <td>1.50</td>
                <td>0.64</td>
                <td>ERR</td>
            </tr>
            <tr>
                <th>A-NeSI</th>
                <td>93.53</td>
                <td>17.14</td>
                <td>10.39</td>
                <td>0.93</td>
                <td>1.21</td>
            </tr>
            <tr>
                <th>CTSketch</th>
                <td>92.17</td>
                <td><strong>83.84</strong></td>
                <td><strong>47.14</strong></td>
                <td><strong>7.76</strong></td>
                <td><strong>2.73</strong></td>
            </tr>
        </tbody>
    </table>
    <table id="addTable" class="styled-table" style="display:none; margin-top:5px">
        <thead>
            <tr>
                <th></th>
                <th>add-1</th>
                <th>add-2</th>
                <th>add-4</th>
                <th>add-15</th>
                <th>add-100</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <th>Scallop</th>
                <td>96.9</td>
                <td>95.3</td>
                <td>TO</td>
                <td>TO</td>
                <td>TO</td>
            </tr>
            <tr>
                <th>DeepSoftLog</th>
                <td><strong>98.4</strong></td>
                <td>96.6</td>
                <td><strong>93.5</strong></td>
                <td><strong>77.1</strong></td>
                <td><strong>25.6</strong></td>
            </tr>
            <tr>
                <th>IndeCateR</th>
                <td>97.7</td>
                <td>93.3</td>
                <td>89.0</td>
                <td>69.6</td>
                <td>ERR</td>
            </tr>
            <tr>
                <th>ISED</th>
                <td>91.4</td>
                <td>93.1</td>
                <td>89.7</td>
                <td>0.0</td>
                <td>0.0</td>
            </tr>
            <tr>
                <th>A-NeSI</th>
                <td>97.4</td>
                <td>96.0</td>
                <td>92.1</td>
                <td>76.8</td>
                <td>ERR</td>
            </tr>
            <tr>
                <th>CTSketch</th>
                <td>98.3</td>
                <td><strong>96.7</strong></td>
                <td>92.5</td>
                <td>74.8</td>
                <td>23.5</td>
            </tr>
        </tbody>
    </table>
    <script>
        function showCustomTable() {
            document.getElementById("sumTable").style.display = "table";
            document.getElementById("addTable").style.display = "none";
        }
        function showMnistArithTable() {
            document.getElementById("sumTable").style.display = "none";
            document.getElementById("addTable").style.display = "table";
        }
        function showMnistOtherTable() {
            document.getElementById("sumTable").style.display = "none";
            document.getElementById("addTable").style.display = "none";
        }
        // Show custom table by default
        showCustomTable();
    </script>
</body>

The baseline methods fail to learn sum-256, whereas CTSketch also learns sum-1024, attaining per-digit accuracy of 93.69%. The next best performer on this task is A-NeSI, whose per-digit accuracy stays at 17.92%. 
Compared to the add-n task, where per-digit superivision is provided, the supervision is only on the final output, and many methods fail due to the weak learning signal. 

<canvas id="myChart" style="width:100%;"></canvas>
<script>
  const data = {
    labels: ["add-100", "visudo", "sudoku", "hwf", "scene", "leaf"],
    datasets: [
      {
        label: 'Scallop',
        data: [0.0, 0.0, 0.0, 96.65, 0.0, 0.0], 
        borderColor: "#B85450",
        backgroundColor: "#F8CECC",
        borderWidth: 1,
      },
      {
        label: 'DeepSoftLog',
        data: [25.6, 0.0, 0.0, 0.0, 0.0, 0.0], 
        borderColor: "#e38820",
        backgroundColor: "#ffcf99",
        borderWidth: 1,
      },
      {
        label: 'IndeCateR',
        data: [0.0, 81.92, 66.50, 95.08, 69.16, 12.72],
        borderColor: "#408bcf",
        backgroundColor: "#99c8f2",
        borderWidth: 1,
      },
      {
        label: 'ISED',
        data: [0.0, 50.0, 80.32, 97.34, 79.95, 68.59],
        borderColor: "#9673A6",
        backgroundColor: "#E1D5E7",
        borderWidth: 1,
      },
      {
        label: 'A-NeSI',
        data: [0.0, 92.11, 26.36, 3.13, 72.40, 61.46], 
        borderColor: "#D6B656",
        backgroundColor: "#FFF2CC",
        borderWidth: 1,
      },
      {
        label: 'CTSketch',
        data: [23.5, 92.5, 81.46, 95.22, 74.55, 69.78], 
        borderColor: "#82B366",
        backgroundColor: "#D5E8D4",
        borderWidth: 1,
      },
    ]
  };
  new Chart(document.getElementById("myChart"), {
    type: "bar",
    data: data,
    options: {
      plugins: {
        legend: {
          display: true,
        },
      },
    }
  });
</script>

We evaluate using 11 tasks from the neurosymbolic learning literature. CTSketch is the best performer on 4 of the task, and always comes within 2.55% to the best performer. 
No other baseline performs as consistently well as CTSketch across the tasks. 
Logic-based methods cannot encode tasks involving GPT-4, whereas sampling-based methods struggle as the number of inputs increase. 
Neural approximation methods struggle when the output space if infinite or symbolic component involves compelx reasoning. 
This demonstrate that although designed for scalability, it is still comparable on variety of classic neurosymbolic tasks. 

**Computational Efficiency**
<!--<div style="margin-bottom:20px">-->
<body>
    <button id="button1" style="background-color: lightgrey" onclick="showAdd15()">add-15</button>
    <button id="button2" style="background-color: lightgrey" onclick="showAdd100()">add-100</button> 
    <canvas width="200" height="130" id="add15-canvas">
      {% include blog_ctsketch_time.html %}
    </canvas>
    <canvas width="200" height="130" id="add100-canvas">
      {% include blog_ctsketch_time2.html %}
    </canvas>
    <script>
        function showAdd15() {
            document.getElementById("add15-canvas").style.display = "flex";
            document.getElementById("add100-canvas").style.display = "none";
        }
        function showAdd100() {
            document.getElementById("add15-canvas").style.display = "none";
            document.getElementById("add100-canvas").style.display = "flex";
        }
        // Show custom table by default
        showAdd15();
    </script>
</body>

We compare test accuracy over training time on two tasks: add-15 and add-100. 
On Add-15, CTSketch takes 1.70 seconds, and IndeCateR, A-NeSI, DSL takes 23.07s, 52.72s, and over 20mins respectively.
On Add-100, CTSketch takes 0.92 seconds per epoch, and converges before DSL even finishes one training epoch.
Due to how efficiently if performs inference, CTSketch learns far faster than the baselines.
There is no additional neural network training requried, nor the expensive proof aggregate steps.
On the other hand, CTSketch prepares the tensor before training, with less than one minute overhead, and training only involves efficient tensor multiplication. 

**Sketching Rank**
<div style="margin-bottom:20px">
<canvas width="200" height="130" id="rank-canvas">
{% include blog_ctsketch_ranking.html %}
</canvas>
</div>
We study how the sketching rank affects accuracy and training time with the HWF task.
We vary the rank for sketching the largest tensor of size $14^7$. 
Comparing the cases of using the original tensor (full-rank) and low-rank approximation, we can see the clear advantage of sketching: when appropriate rank is chosen, CTSketch converges much faster without sacrificng accuracy.
While the rank have to be sufficiently large to learn the optimal weights, the algorithm is not particularly sensitive to the choice of rank, and can be chosen flexibly depending on the available resources.

## Limitations and Future Work

The primary limitation of CTSketch lies in requiring manual decomposition of the symbolic part to scale to tasks with large number on inputs. 
For future work, we are interested in automating the decomposition, possibly using program synthesis techniques.

Another interesting future direction is exploring different tensor sketching methods and trade-offs they provide. If we use a streaming algorithm, for example, we would significantly reduce the memory usage with a small time overhead when initializing the tensor sketches.


## Conclusion
We proposed CTSketch, a framework that uses decomposed programs to scale neurosymbolic learning.
CTSketch uses sketched tensors representing the summary of each sub-program to efficiently approximate the output distribution of the symbolic component using simple tensor operations.
We demonstrate that CTSketch pushes the frontier of neurosymbolic learning, solving significantly larget problem then prior works could solve, while remaining competitive with existing techniques on standard neurosymbolic learning benchmarks.

For more details about our method and experiments, see our [paper](https://arxiv.org/abs/) and [code](https://github.com/alaiasolkobreslin/CTSketch).


### Citation
```
@article{choi2025CTSketch,
  title={CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning},
  author={Choi, Seewon and Solko-Breslin, Alaia and Alur, Rajeev and Wong, Eric},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```