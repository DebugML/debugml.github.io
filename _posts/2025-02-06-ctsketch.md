---
title: "CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning"
layout: single
excerpt: "Scaling neurosymbolic learning with program decomposition and sketching"
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: assets/images/neural_programs/scene_recognition.png
  teaser: assets/images/neural_programs/scene_recognition.png
  actions:
    - label: "Paper"
      url: https://arxiv.org/abs/XXXX.XXXX
    - label: "Code"
      url: https://github.com/alaiasolkobreslin/CTSketch
authors:
  - Seewon Choi|equal
  - Alaia Solko-Breslin|equal
  - Rajeev Alur
  - Eric Wong

scene_recognition:
  - url: /assets/images/neural_programs/scene_recognition.png
    image_path: /assets/images/neural_programs/scene_recognition.png
    title: Scene recognition can be decomposed as an object detector followed by a call to GPT-4 to classify the scene.

neural_programs:
  - id: 0
    name: sum-n
    caption: Program Decomposition for MNIST sum of n digits
  - id: 1
    name: visudo
    caption: Program Decomposition for Visual Sudoku
  - id: 2
    name: add-n
    caption: Program Decomposition for MNIST addition of 2 n-digit numbers
  - id: 3
    name: sudoku
    caption: Program Decomposition for Sudoku Solving

---
<style>
.histogram-row {
    display: flex;
    justify-content: space-between;
    flex-wrap: nowrap;
}

.histogram-row > * {
    flex: 0 0 48%; /* this ensures the child takes up 48% of the parent's width (leaving a bit of space between them) */
}

.button-method {
  width: 25%;
  background: rgba(76, 175, 80, 0.0);
  border: 0px;
  border-right: 1px solid #ccc;
  color: #999;
}

.button-sample {
  padding: 5px;
  font-size: 12px;
  background: rgba(76, 175, 80, 0.0);
  display: inline-block;
  margin-right: 15px;
}

.btn-clicked {
  color: black;
}

.container {
  display: flex;
  overflow: auto;
  align-items: center;
}

.container th, .container td {
  text-align: center;
  padding: 1px 5px;
}

.container table {
  width: auto; 
  padding-top:15px;
  margin-right: 5px;
}

.container math, .container div {
  width: auto; 
  margin-right: 15px;
}

.container div {
  margin-left: 15px;
}

.code-block {
  font-size: 14px; /* Adjust the font size as needed */
  text-align: left;
}

.code-snippet {
  display: inline-block;
  margin-left: 15px;
  margin-right: 15px;
}

</style>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>


> This post introduces CTSketch, an algorithm for learning tasks expressed as the composition of a neural network followed by a symbolic program (neurosymbolic learning).
> CTSketch uses tensors to summarize the input-output pairs of sub-programs of the decomposed symbolic program, and performs fast inference via simple tensor operations.
> CTSketch pushes the frontier of neursymbolic learning, scaling to tasks involving 1,024 inputs, which has never been done before. 

Neurosymbolic programs are the composition of a neural network $M_\theta$ followed by a symbolic program $P$.
The challenge of neurosymbolic learning concerns learning $M_\theta$ without intermediate labels.
Concretely, given an input example $(x, y)$, symbolic program $c$, and loss function $\mathcal{L}$, we must predict the distribution for the output $\hat{y} = c(M_{\theta}(x))$ while ensuring thta the loss $\mathcal{L}(\hat{y}, y)$ is fully differentiable.
We now discuss prior works on neurosymbolic learning which provide different techniques for estimating the output distribution of the composite.

## White- and Black-Box Neurosymbolic Frameworks

Existing neurosymbolic learning techniques can usually be categorized as either white- or black-box.
White-box techniques allow access to the internals of the symbolic program in order to compute and output probability distribution from the input distributions.
Differentiable logic programming frameworks are one form of white-box learning techniques.
These frameworks require the symbolic component to be written in a logic programming language like probabilistic Prolog (as in [DeepProbLog](https://arxiv.org/abs/1805.10872)) or Datalog (as in [Scallop](https://arxiv.org/abs/2304.04812)).

On the other hand, black-box frameworks assume no access to the internals of the symbolic component.
[IndeCateR](https://arxiv.org/abs/2311.12569) and [ISED](https://arxiv.org/abs/2406.06246) are two frameworks that sample inputs to the symbolic program and reward inputs that result in the correct output.
A-NeSI is an algorithm for approximating the output distribution of the composite with a neural network.
Because these techniques are suited for black-box programs, the symbolic component can be written in any programming language.
This means that the symbolic component can even use a large language model (LLM) like GPT-4 to perform reasoning over the predictions from the network.
For example, the task of scene recognition has a natural decomposition of an object detector followed by a program that prompts GPT-4 to classify the scene based on these objects.

{% include gallery id="scene_recognition" caption="Scene recognition can be decomposed as an object detector followed by a program that prompts GPT-4 to classify the scene based on the predicted objects." %}

## CTSketch

We introduce CTSketch, a novel framework that combines the strengths of white- and black-box techniques.
CTSketch uses two techniques to improve the scalability of neurosymbolic inference: decompose the program into multiple sub-program and ...

<!-- scales to tasks involving many inputs  -->


## Evaluation


## Limitations and Future Work


## Conclusion


### Citation
```
@article{choi2025CTSketch,
  title={CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning},
  author={Choi, Seewon and Solko-Breslin, Alaia and Alur, Rajeev and Wong, Eric},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```