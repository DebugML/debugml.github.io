---
published: true
title:  "Where’s the Bug? Attention Probing for Scalable Fault Localization"
excerpt: "Attention probing LLMs allows for accurately locating bugs in code without direct labels, code execution, or the largest of LLMs."
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: assets/images/BAP/waldo.jpg
  teaser: assets/images/BAP/bap_fig1.png
  actions:
    - label: "Paper"
      url: "https://arxiv.org/abs/2502.13966"
    - label: "Code"
      url: "https://github.com/adaminsky/BAP" 
authors:
  - Adam Stein|equal
  - Arthur Wayne|equal
  - Aaditya Naik
  - Mayur Naik
  - Eric Wong

step1:
  - url: "/assets/images/BAP/codeprobes_1_blog.png"
    image_path: "/assets/images/BAP/codeprobes_1_blog.png"
    alt: ""
    title: ""
step2:
  - url: "/assets/images/BAP/codeprobes_2_blog.png"
    image_path: "/assets/images/BAP/codeprobes_2_blog.png"
    alt: ""
    title: ""
step3:
  - url: "/assets/images/BAP/codeprobes_3_blog.png"
    image_path: "/assets/images/BAP/codeprobes_3_blog.png"
    alt: ""
    title: ""
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script src="https://cdn.jsdelivr.net/npm/chart.js@3.7"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-chart-matrix@1.1"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>

> Locating bugs in code remains a challenging problem for both humans and automated systems.
> Existing methods either require existing test cases, training on costly line-level annotations, or resource-intensive LLMs. We present Bug Attention Probe (BAP), a method which learns to localize bugs without any direct localization labels, outperforming traditional fault localization baselines and prompting of large-scale LLMs. BAP is also highly efficient, outperforming large open-weight models at a small fraction of the computational cost.

Pinpointing the precise location of bugs in code, also known as fault localization (FL), is a central challenge in software engineering. As large language models (LLMs) become increasingly performant on code-related tasks, automating FL becomes even more useful and important. For example, LLMs are now used to propose complete bug fixes from only a user’s bug report, but their effectiveness is <i>fundamentally limited</i> by their ability to first locate the bug.

In the field of software engineering, this problem has been extensively studied, but the proposed methods are often limited by the requirement for code execution and results in overall poor performance. Rather than rely on code execution, deep learning approaches train models on line-level supervision (which lines have a bug and which do not have a bug), but this finegrained supervision is costly to collect. On the other hand, the largest and most capable LLMs now achieve state-of-the-art FL through just prompting.

For large, real-world, and rapidly evolving codebases, repeated use of expensive LLMs becomes highly costly, so it is valuable to achieve strong FL with smaller models. The leads us to our central question:

How do we achieve strong FL performance without tests, strong supervision, or large-scale models?
{: .notice--info}

To this end, we propose the *Bug Attention Probe* (BAP) which learns to perform FL using only *weak supervision*—that is, without needing line-level localization labels. Before presenting BAP, we first introduce the problem of FL.

## Fault Localization (FL)
Before we start *finding* bugs, we define what a bug is in the first place. For our purposes, we say that a code segment can either have a bug or not have a bug, so we define a bug as a concept $$b: \mathcal{P}\rightarrow \{0, 1\}$$ where $$\mathcal{P}$$ is the space of all possible programs.

We define what it means to localize a bug in terms of a counterfactual explanation. In particular, a program has a bug localized to line $$i$$ if modifying only line $$i$$ would remove the bug. Not all bugs can be localized to a single line, meaning one must modify multiple lines of code to make a code segment bug free. Therefore, the groundtruth FL labels for buggy code fragments consist of one or more numbers corresponding to the lines of code which must be modified to remove the bug.

An FL method is a function which takes a buggy code fragment as input and outputs one or more line numbers, $$l: \mathcal{P}\rightarrow \mathbb{Z}^+$$. An FL method can be extremely useful since it directly outputs which lines of code should be modified to fix a bug, but performing FL in practice faces several challenges.

### Challenges in FL

We identify three main challenges faced by existing FL methods.

1. **Need for Strong Supervision**: Learning-based FL methods train on datasets consisting of code fragments paired with the fault localization line numbers. Such datasets, however, require large amounts of manual effort to create since it requires already having a working method for fault localization. It is instead much easier to collect large amounts of code labelled as buggy or not since there are many accurate bug detection methods such as failing tests.
2. **Localizing Multi-line Bugs**: Most real-world bugs are localized to multiple lines, but existing FL methods mostly focus on the single-line case.
3. **Resource Efficiency**: LLMs perform state-of-the-art for FL, but only the most powerful LLMs which are often very expensive, especially for repeated calls.

To address these challenges, we next present our method based on LLM probing.


## Attention Probing for FL

BAP is built around the insight that an LLM's attention mechanism can implicitly reveal the most suspicious parts of code. If we train a small model to classify code as buggy or not (which is weak supervision for the task of FL), its learned attention weights may reveal which tokens lead to that decision.
Summing those attention patterns at the line level produces a human-interpretable localization—no explicit line-level bug label needed.

BAP consists of 3 steps: training with weak supervision, token-level attention extraction, and line-level attention aggregation. Click through the tabs below for an explanation of each step.

<div class="tabs">
  <input type="radio" id="tab1" name="tab-control" checked>
  <input type="radio" id="tab2" name="tab-control">
  <input type="radio" id="tab3" name="tab-control">
  
  <ul>
    <li title="Weak Supervision">
      <label for="tab1" role="button">
        <span>1. Weak Supervision</span>
      </label>
    </li>
    <li title="Attention Extraction">
      <label for="tab2" role="button">
        <span>2. Attention Extraction</span>
      </label>
    </li>
    <li title="Line-level Ranking">
      <label for="tab3" role="button">
        <span>3. Line-level Ranking</span>
      </label>
    </li>
  </ul>
  
  <div class="slider"><div class="indicator"></div></div>
  <div class="content">
    <section>
      <p>BAP trains on a bug detection dataset: each sample is labeled just "buggy" or "clean." No line-level info needed.</p>
      <img src="/assets/images/BAP/codeprobes_1.png" alt="Weak Supervision">
    </section>
    <section>
      <p>We attach a single-layer Transformer decoder block (a probe) to the LLM's hidden states. As we train on the detection task, we collect each token's attention score to the final token.</p>
      <img src="/assets/images/BAP/codeprobes_2.png" alt="Attention Extraction">
    </section>
    <section>
      <p>We sum the attention of tokens belonging to each line. The line(s) with the highest sum is flagged as most likely containing the bug.</p>
      <img src="/assets/images/BAP/codeprobes_3.png" alt="Line-level Ranking">
    </section>
  </div>
</div>

<style>
.tabs {
  position: relative;
  margin: 2rem 0;
  background: #fff;
  height: auto;
  min-height: 26em;
  border-radius: 8px;
  box-shadow: 0 2px 10px rgba(0,0,0,0.05);
}

.tabs input[type=radio] {
  position: absolute;
  opacity: 0;
  z-index: -1;
}

.tabs ul {
  list-style: none;
  padding: 0;
  margin: 0;
  display: flex;
  justify-content: space-around;
  position: relative;
  border-bottom: 2px solid #f0f0f0;
}

.tabs ul li {
  flex: 1;
}

.tabs ul li label {
  display: block;
  padding: 1rem;
  background: #fafafa;
  cursor: pointer;
  text-align: center;
  font-weight: bold;
  transition: background 0.3s, color 0.3s;
  border-radius: 8px 8px 0 0;
}

.tabs ul li label:hover {
  background: #f0f0f0;
}

.tabs input[type=radio]:checked + label {
  background: #fff;
  color: #007bff;
  border-bottom: 3px solid #007bff;
}

.tabs .slider {
  position: absolute;
  top: 0;
  left: 0;
  width: 33.33%;
  height: 4px;
  background: #007bff;
  transition: transform 0.3s ease;
  z-index: 5;
}

.tabs .content {
  position: relative;
  background: #fff;
  padding: 1.5rem;
  border-radius: 0 0 8px 8px;
  min-height: 300px;
}

.tabs .content section {
  display: none;
  animation: fadeIn 0.6s ease;
  position: relative;
  width: 100%;
  overflow-y: auto;
  max-height: 600px;
}

@keyframes fadeIn {
  from { opacity: 0; transform: translateY(5px); }
  to { opacity: 1; transform: translateY(0); }
}

.tabs input[type=radio]:nth-of-type(1):checked ~ .slider {
  transform: translateX(0%);
}

.tabs input[type=radio]:nth-of-type(2):checked ~ .slider {
  transform: translateX(100%);
}

.tabs input[type=radio]:nth-of-type(3):checked ~ .slider {
  transform: translateX(200%);
}

.tabs input[type=radio]:nth-of-type(1):checked ~ .content section:nth-of-type(1),
.tabs input[type=radio]:nth-of-type(2):checked ~ .content section:nth-of-type(2),
.tabs input[type=radio]:nth-of-type(3):checked ~ .content section:nth-of-type(3) {
  display: block;
}

@media (max-width: 768px) {
  .tabs ul {
    flex-direction: column;
  }
  
  .tabs .slider {
    display: none;
  }
  
  .tabs ul li label {
    border-radius: 0;
  }
  
  .tabs input[type=radio]:checked + label {
    background-color: #e6f2ff;
    color: #007bff;
    border-left: 4px solid #007bff;
    font-weight: 700;
    padding-left: calc(1rem - 4px);
  }
}
</style>






## A Toy Example
We will now demonstrate BAP on a simple example to illustrate how it works and its usefulness.
Consider the following Java code snippet which we wrote:
```java
Integer vote;
public void addVote(int age) {
  if (age >= 18) {
    System.out.println("You're a minor!");
  } else {
    vote++;
    System.out.println("You can vote!");
  }
}
```
If you look closely at the code, there are two major bugs! The first is that the condition for the if statement is inverted (`>= 18` prints "You’re a minor!"). The second issue is slightly more subtle in that the `vote` variable is defined as an `Integer` object, but the constructor is never called. This means that `vote` will be null when we try to increment its value. How would we apply BAP to this code and what will the output look like?

### BAP Attends to Buggy Lines
<!-- BEGIN HIGHLIGHTED CODE SNIPPET -->
<style>
.code-container {
    font-family: monospace;
    font-size: 16px;
    padding: 5px;
    background-color: #f5f5f5;
    border-radius: 3px;
    /* width: 500px; */
    border: 1px solid #ddd;
}
.line-container {
    display: flex;
    align-items: flex-start;
}
.score {
    width: 40px;
    text-align: right;
    padding-right: 10px;
    color: #666;
    font-weight: bold;
    font-size: 16px;
}
.code {
    flex: 1;
    padding-left: 3px;
    transition: background-color 0.2s, color 0.2s;
    white-space: pre;
}
.buggy:hover {
    background-color: rgba(255, 0, 0, 0.4) !important;
    color: white;
    cursor: pointer;
}
</style>

<div class='code-container'>

<div class='line-container'><span class='score'>0.03</span><span class='code' style='background-color: rgba(0, 204, 102, 0.05);'>Integer vote;</span></div>
<div class='line-container'><span class='score'>0.07</span><span class='code' style='background-color: rgba(0, 204, 102, 0.10);'>public void addVote(int age) {</span></div>
<div class='line-container'><span class='score' style='color: red'>0.28</span><span class='code buggy' style='background-color: rgba(0, 204, 102, 0.66666666);'>    if (age >= 18) {</span></div>
<div class='line-container'><span class='score'>0.05</span><span class='code' style='background-color: rgba(0, 204, 102, 0.03);'>        System.out.println("You're a minor!");</span></div>
<div class='line-container'><span class='score'>0.04</span><span class='code' style='background-color: rgba(0, 204, 102, 0.02);'>    } else {</span></div>
<div class='line-container'><span class='score' style='color: red'>0.28</span><span class='code buggy' style='background-color: rgba(0, 204, 102, 0.66666666);'>        vote++;</span></div>
<div class='line-container'><span class='score'>0.10</span><span class='code' style='background-color: rgba(0, 204, 102, 0.15);'>        System.out.println("You can vote!");</span></div>
<div class='line-container'><span class='score'>0.02</span><span class='code' style='background-color: rgba(0, 204, 102, 0.0);'>    }</span></div>
<div class='line-container'><span class='score'>0.01</span><span class='code' style='background-color: rgba(0, 204, 102, 0.0);'>}</span></div>

</div>
<!-- END HIGHLIGHTED CODE SNIPPET -->


After training on code examples labelled with just "buggy" or "clean," BAP’s line-level attention highlights `if (age >= 18)` and `vote++;` as top suspects for buggy lines. No test harness or line-level annotations were needed—only knowledge of what buggy and non-buggy code snippets tend to look like.

In reality, the bugs we evaluate on are far less trivial and better resemble real-world bug encounters.


## BAP Evaluation
We evaluate BAP on eight diverse fault localization datasets including Python, Java, and C bugs, and we show that BAP outperforms the baselines at top-1 FL performance. See the first tab below for the plot of top-1 FL accuracy for all eight datasets of BAP compared to GPT-4o prompting as well as a recent deep learning FL approach called LLMAO.


Because our method produces a ranking for each line of code in a given snippet, multiple suspicious lines can be identified simultaneously.

BAP is significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost. Even a 1B parameter base model with BAP outperforms zero-shot prompts on 90B parameter models and GPT-4o.



<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>All Charts Example</title>
  <!-- Load Chart.js only once -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
</html>


<div class="tabs" style="height: 34em">
  <input type="radio" id="tab4" name="tab-control1" checked>
  <input type="radio" id="tab5" name="tab-control1">
  <input type="radio" id="tab6" name="tab-control1">
  <ul>
    <li title="Top-1 Localization Accuracy"><label for="tab4" role="button"><span>Localization Accuracy</span></label></li>
    <li title="Multiple Lines Precision"><label for="tab5" role="button"><span>Multi-line Bug Precision</span></label></li>
    <li title="Resource Efficiency"><label for="tab6" role="button"><span>Resource Efficiency</span></label></li>
  </ul>

  <div class="slider"><div class="indicator"></div></div>
  <div class="content">
    <section>
      <!-- <h2>Localization Accuracy</h2> -->
      <p>
        Averaged across all eight datasets, BAP improves by 34.6% top-1 accuracy
        compared to the strongest baseline and 93.4% over zero-shot prompting GPT-4.
      </p>
      <canvas id="comparisonChart" width="800" height="600"></canvas>
      <script>
        const ctx1 = document.getElementById('comparisonChart').getContext('2d');
        const data1 = {
          labels: ['D4J', 'GH-Py', 'GH-J', 'DeepFix', 'TSSB', 'MS4J', 'Juliet-J', 'Juliet-C', 'Avg.'],
          datasets: [
            {
              label: 'BAP',
              data: [0.334, 0.575, 0.568, 0.481, 0.327, 0.291, 0.096, 0.217, 0.350],
              backgroundColor: 'darkblue'
            },
            {
              label: 'LLMAO',
              data: [0.144, 0.109, 0.122, 0.118, 0.116, 0.063, 0.113, 0.126, 0.126],
              backgroundColor: 'skyblue'
            },
            {
              label: 'GPT-4o',
              data: [0.240, 0.375, 0.365, 0.240, 0.009, 0.240, 0.009, 0.026, 0.181],
              backgroundColor: '#74AA9C'
            }
          ]
        };
        const config1 = {
          type: 'bar',
          data: data1,
          options: {
            responsive: true,
            plugins: {
              legend: { position: 'top' },
              title: { display: true, text: 'Fault Localization Performance' }
            },
            scales: {
              x: {
                title: { display: true, text: 'Benchmark' }
              },
              y: {
                title: { display: true, text: 'Top-1 Accuracy' },
                min: 0.0,
                max: 0.6
              }
            }
          }
        };
        Chart.defaults.font.size = 16;
        new Chart(ctx1, config1);
      </script>
    </section>
    <section>
      <!-- <h2>Multiple Lines Precision</h2> -->
      <p>
        On multi-line bugs, BAP captures more relevant lines than typical single-line
        localizers, showing better precision at top-k lines.
      </p>
      <canvas id="barChart" width="800" height="600"></canvas>
      <script>
        const ctx2 = document.getElementById('barChart').getContext('2d');
        const data2 = {
          labels: ['P@2', 'P@3', 'P@5'],
          datasets: [
            {
              label: 'Random',
              data: [0.201, 0.231, 0.297],
              backgroundColor: 'gray'
            },
            {
              label: 'CodeLlama-70B',
              data: [0.250, 0.284, 0.351],
              backgroundColor: 'skyblue'
            },
            {
              label: 'Llama-3.3-70B',
              data: [0.240, 0.266, 0.355],
              backgroundColor: 'royalblue'
            },
            {
              label: 'Qwen2.5-72B',
              data: [0.221, 0.271, 0.347],
              backgroundColor: 'SlateBlue'
            },
            {
              label: 'DeepSeek-R1-Distill-Llama-70B',
              data: [0.245, 0.283, 0.336],
              backgroundColor: '#4169E1'
            },
            {
              label: 'GPT-4o',
              data: [0.218, 0.288, 0.359],
              backgroundColor: '#74AA9C'
            },
            {
              label: 'BAP-Llama-3.2-11B',
              data: [0.289, 0.298, 0.367],
              backgroundColor: 'darkblue'
            }
          ]
        };
        const config2 = {
          type: 'bar',
          data: data2,
          options: {
            responsive: true,
            plugins: {
              legend: { position: 'top' },
              title: { display: true, text: 'Precision at Top-K Lines' }
            },
            scales: {
              x: { title: { display: true, text: 'Top-K Lines' } },
              y: {
                title: { display: true, text: 'Precision' },
                min: 0.15,
                max: 0.4
              }
            }
          }
        };
        new Chart(ctx2, config2);
      </script>
    </section>
    <section>
      <!-- <h2>Resource Efficiency</h2> -->
      <p>
        BAP’s smaller memory requirements and lower FLOPs mean it can be integrated
        into continuous integration workflows or potentially even local development
        IDEs.
      </p>
      <canvas id="scatterChart" width="800" height="600"></canvas>
      <script>
        const ctx3 = document.getElementById('scatterChart').getContext('2d');
        const data3 = {
          datasets: [
            {
              label: 'BAP',
              data: [
                { x: 6.2,  y: 0.282, label: 'BAP-Llama-3.2-1B'  },
                { x: 24.2, y: 0.334, label: 'BAP-Llama-3.2-11B' }
              ],
              backgroundColor: 'royalblue',
              pointStyle: 'triangle',
              pointRadius: 9,
              borderColor: 'black',
              borderWidth: 1
            },
            {
              label: 'Zero-Shot',
              data: [
                { x: 170.0, y: 0.269, label: 'Llama-3.2-90B' },
                { x: 131.7, y: 0.212, label: 'CodeLlama-70B' },
                { x: 138.9, y: 0.157, label: 'Qwen2.5-72B' },
                { x: 154.0, y: 0.269, label: 'Llama-3.3-70B' },
                { x: 141.2, y: 0.221, label: 'DeepSeek-R1-Llama-3.3-70B' }
              ],
              backgroundColor: 'red',
              pointStyle: 'star',
              pointRadius: 8,
              borderColor: 'red',
              borderWidth: 2
            }
          ]
        };
        const config3 = {
          type: 'scatter',
          data: data3,
          options: {
            responsive: true,
            plugins: {
              legend: { position: 'top' },
              title: { display: true, text: 'Top-1 Accuracy vs GPU Cost' },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    let dataPoint = context.raw;
                    return `${dataPoint.label}: (${dataPoint.x}, ${dataPoint.y})`;
                  }
                }
              }
            },
            scales: {
              x: { title: { display: true, text: 'GPU Cost (GB)' } },
              y: { title: { display: true, text: 'Top-1 Accuracy' } }
            }
          }
        };
        new Chart(ctx3, config3);
      </script>
    </section>
  </div>
</div>


## Future Directions

- **Long-Code FL**: For functions spanning 100+ lines, BAP could be extended with different aggregation functions or improved attention mechanisms.
- **Repository-Level Localization**: BAP currently focuses on method-level context; next steps involve bridging to entire files or multi-file projects.
- **Broader Context**: Combining BAP with bug-fixing models or test generation frameworks might yield an end-to-end debugging pipeline.


## Conclusion

BAP demonstrates that weak supervision can yield strong line-level fault localization. It bridges the gap between requiring test-based FL and prompting large language models. If you’re searching for a simple, efficient, and performant way to spot code bugs, give BAP a look!

Check out our paper and code if interested:
- **[Read the Paper](https://arxiv.org/abs/2502.13966)** for rigorous experiments, ablations, and theoretical underpinnings.  
- **[Check Out the Code](https://github.com/adaminsky/BAP)** to try BAP’s bug localization on your own projects.